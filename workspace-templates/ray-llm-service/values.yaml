# Ray LLM Service Helm Chart Values
global:
  deploymentName: "my-llm-service"
  deploymentNamespace: "default"

# DO NOT CHANGE THIS PARAMETER
deploymentNumReplicas: 1

# Deployment configuration
deploymentImage: "rayproject/ray-ml:2.46.0.0e19ea"

# If you require access to access-restricted models, you can specify a Hugging Face token here.
# huggingfaceToken: "my-hugging-face-token"

# LLM / Ray / vLLM specific configuration
numModelReplicas: 1
runtimeEnvironmentPipPackages: "numpy==1.26.4,vllm==0.9.0,ray==2.46.0"

huggingfaceModel: "microsoft/phi-4"
tensorParallelSize: 1
pipelineParallelSize: 1
placementGroupStrategy: "PACK"
cpuPerActor: 16
gpuPerActor: 1

# Ephemeral storage configuration
ephemeralStorageGb: 50

# Resource configuration (this maps to ResourcePool spec in API, but camelcase)
# the workspace template does not configure pvc storage
cpuCores: 16
memoryGb: 32
gpuCount: 1
