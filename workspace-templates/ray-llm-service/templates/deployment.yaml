apiVersion: ray.io/v1
kind: RayService
metadata:
  name: {{ include "ray-llm-service.deploymentName" . }}
  namespace: {{ .Values.global.deploymentNamespace | default "default" }}
spec:
  serveConfigV2: |
    applications:
    - name: llm
      route_prefix: /
      import_path: llm_deployment:model
      deployments:
      - name: LLMDeployment
        num_replicas: {{ .Values.numModelReplicas }}
      runtime_env:
{{- if .Values.runtimeEnvironmentPipPackages }}
        pip:
{{- $items := split "," .Values.runtimeEnvironmentPipPackages }}
{{- range $i, $item := $items }}
          - {{ $item | quote }}
{{- end }}
{{- end }}
        env_vars:
          DYNAMIC_RAY_CLI_ARG_TENSOR_PARALLEL_SIZE: "{{ .Values.tensorParallelSize }}"
          DYNAMIC_RAY_CLI_ARG_PIPELINE_PARALLEL_SIZE: "{{ .Values.pipelineParallelSize }}"
          BUILD_APP_ARG_PLACEMENT_GROUP_STRATEGY: "{{ .Values.placementGroupStrategy }}"
          BUILD_APP_ARG_CPU_PER_ACTOR: "{{ .Values.cpuPerActor }}"
          BUILD_APP_ARG_GPU_PER_ACTOR: "{{ .Values.gpuPerActor }}"
          DYNAMIC_RAY_CLI_ARG_MODEL: "{{ .Values.huggingfaceModel }}"
          DYNAMIC_RAY_CLI_ARG_SERVED_MODEL_NAME: "{{ .Values.huggingfaceModel }}"
  rayClusterConfig:
    headGroupSpec:
      rayStartParams:
        dashboard-host: 0.0.0.0
      template:
        metadata:
          labels:
            component.type: head
            component.subtype: serve-endpoint
            ray.name: {{ include "ray-llm-service.deploymentName" . }}
            workspace.exalsius.ai/database-id: {{ .Release.Name }}
        spec:
{{- if and .Values.resources.gpuCount (gt (int .Values.resources.gpuCount) 0) }}
          runtimeClassName: nvidia
{{- end }}
          containers:
            - image: {{ .Values.deploymentImage }}
              name: ray-head
              ports:
                - containerPort: 6379
                  name: gcs-server
                  protocol: TCP
                - containerPort: 8265
                  name: dashboard
                  protocol: TCP
                - containerPort: 10001
                  name: client
                  protocol: TCP
                - containerPort: 8000
                  name: serve
                  protocol: TCP
              env:
              - name: PYTHONPATH
                value: /tmp/files
{{- if .Values.huggingfaceToken }}
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: {{ include "ray-llm-service.secretName" . }}
                    key: hf_api_token
{{- end }}
              resources:
                requests:
                  ephemeral-storage: 5Gi
                  cpu: {{ .Values.resources.cpuCores }}
                  memory: {{ .Values.resources.memoryGb }}Gi
                  nvidia.com/gpu: {{ .Values.resources.gpuCount }}
                limits:
                  ephemeral-storage: 10Gi
                  cpu: {{ .Values.resources.cpuCores }}
                  memory: {{ .Values.resources.memoryGb }}Gi
                  nvidia.com/gpu: {{ .Values.resources.gpuCount }}
              volumeMounts:
              - mountPath: /tmp/files
                name: ray-head-files
          volumes:
          - name: ray-head-files
            configMap:
              name: {{ include "ray-llm-service.configmapName" . }}
  