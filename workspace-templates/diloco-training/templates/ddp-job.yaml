apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: {{ include "diloco-training.jobName" . }}
  namespace: {{ .Values.global.deploymentNamespace }}
  labels:
    job-name: {{ include "diloco-training.jobName" . }}
spec:
  maxRetry: 3
  minAvailable: {{ include "diloco-training.totalMinNodes" . }}
  plugins:
    svc: []
  queue: default
  schedulerName: volcano
  tasks:
{{- if .Values.gpu.nvidia.enabled }}
    - maxRetry: 3
      minAvailable: {{ include "diloco-training.nvidia.minNodes" . }}
      name: nvidia-worker
      replicas: {{ include "diloco-training.nvidia.maxNodes" . }}
      template:
        metadata:
          annotations:
            scheduling.k8s.io/group-name: {{ include "diloco-training.jobName" . }}
          labels:
            job-name: {{ include "diloco-training.jobName" . }}
            workspace.exalsius.ai/database-id: {{ .Release.Name }}
        spec:
          runtimeClassName: {{ .Values.gpu.nvidia.runtimeClassName }}
          containers:
            - command:
                - /bin/sh
                - -c
                - |
                  set -e
                  # Ensure output is not buffered
                  export PYTHONUNBUFFERED=1
                  export TORCH_DISTRIBUTED_DEBUG=INFO
                  
                  # NCCL configuration for high-latency environments
                  export NCCL_DEBUG={{ .Values.elastic.nccl.debug | default "WARN" }}
                  export NCCL_ASYNC_ERROR_HANDLING={{ .Values.elastic.nccl.asyncErrorHandling | default 1 }}
                  export NCCL_TIMEOUT={{ .Values.elastic.nccl.timeout | default 600 }}
                  export NCCL_IB_TIMEOUT={{ .Values.elastic.nccl.ibTimeout | default 22 }}
                  export NCCL_NSOCKS_PERTHREAD={{ .Values.elastic.nccl.nsocksPerThread | default 8 }}
                  export NCCL_SOCKET_NTHREADS={{ .Values.elastic.nccl.socketNthreads | default 4 }}
                  {{- if .Values.elastic.nccl.socketIfname }}
                  export NCCL_SOCKET_IFNAME={{ .Values.elastic.nccl.socketIfname }}
                  {{- end }}
                  echo "NCCL configured for high-latency environment (DEBUG=${NCCL_DEBUG})" >&2
                  
                  echo "Starting PyTorch Elastic training..." >&2
                  echo "POD_NAME: ${POD_NAME}" >&2
                  
                  # Determine etcd rendezvous endpoint
                  # etcd provides fault-tolerant coordination for PyTorch Elastic training
                  echo "Configuring etcd rendezvous backend..." >&2
                  
                  {{- if .Values.elastic.etcd.externalEndpoint }}
                  # Using external etcd cluster
                  ETCD_ENDPOINT="{{ .Values.elastic.etcd.externalEndpoint }}"
                  echo "Using external etcd: ${ETCD_ENDPOINT}" >&2
                  {{- else if .Values.etcd.enabled }}
                  # Using embedded etcd deployed with this chart
                  ETCD_HOST="{{ include "diloco-training.etcdEndpoint" . }}"
                  ETCD_ENDPOINT="${ETCD_HOST}"
                  echo "Using embedded etcd: ${ETCD_ENDPOINT}" >&2

                  # Wait for etcd to be ready
                  echo "Waiting for etcd to be ready..." >&2
                  ETCD_READY=false
                  for i in $(seq 1 30); do
                    # Try to connect to etcd health endpoint
                    if command -v curl > /dev/null 2>&1; then
                      if curl -sf --connect-timeout 2 "{{ .Values.elastic.etcd.protocol | default "http" }}://${ETCD_ENDPOINT}/health" > /dev/null 2>&1; then
                        echo "etcd is ready and healthy (via curl)" >&2
                        ETCD_READY=true
                        break
                      fi
                    elif command -v wget > /dev/null 2>&1; then
                      if wget -q --timeout=2 --tries=1 "{{ .Values.elastic.etcd.protocol | default "http" }}://${ETCD_ENDPOINT}/health" -O /dev/null 2>&1; then
                        echo "etcd is ready and healthy (via wget)" >&2
                        ETCD_READY=true
                        break
                      fi
                    elif command -v python3 > /dev/null 2>&1; then
                      if python3 -c "import urllib.request; urllib.request.urlopen('{{ .Values.elastic.etcd.protocol | default "http" }}://${ETCD_ENDPOINT}/health', timeout=2)" > /dev/null 2>&1; then
                        echo "etcd is ready and healthy (via python)" >&2
                        ETCD_READY=true
                        break
                      fi
                    fi
                    echo "Waiting for etcd to be ready... (attempt $i/30)" >&2
                    sleep 2
                  done
                  
                  if [ "$ETCD_READY" = "false" ]; then
                    echo "WARNING: Could not verify etcd availability after 30 attempts" >&2
                    echo "Continuing anyway - torchrun will handle connection retry" >&2
                  fi
                  
                  # Clean up stale etcd data from previous runs
                  echo "Cleaning up stale etcd data..." >&2
                  if command -v etcdctl > /dev/null 2>&1; then
                    ETCD_PREFIX="{{ .Values.elastic.etcd.prefix | default "/torchelastic" }}"
                    echo "Attempting to remove prefix: ${ETCD_PREFIX}" >&2
                    ETCDCTL_API=2 etcdctl --endpoints=${ETCD_ENDPOINT} rm --recursive ${ETCD_PREFIX} 2>&1 | grep -v "Key not found" || true
                    echo "Cleanup complete" >&2
                  else
                    echo "etcdctl not available, skipping cleanup (etcd will handle conflicts)" >&2
                  fi
                  {{- else }}
                  echo "ERROR: etcd is not enabled and no external endpoint specified!" >&2
                  exit 1
                  {{- end }}
                  
                  # Use etcd endpoint (hostname:port only, path goes in rdzv-conf)
                  RDZV_ENDPOINT="${ETCD_ENDPOINT}"
                  
                  echo "etcd rendezvous endpoint: ${RDZV_ENDPOINT}" >&2
                  echo "Node rank ${NODE_RANK} ready for fault-tolerant elastic training" >&2
                  
                  # Check if torchrun exists (try multiple locations)
                  TORCHRUN_CMD=""
                  if command -v torchrun &> /dev/null; then
                    TORCHRUN_CMD="torchrun"
                  elif [ -f "/app/.venv/bin/torchrun" ]; then
                    TORCHRUN_CMD="/app/.venv/bin/torchrun"
                  elif [ -f "/usr/local/bin/torchrun" ]; then
                    TORCHRUN_CMD="/usr/local/bin/torchrun"
                  else
                    echo "ERROR: torchrun command not found!" >&2
                    echo "Available in PATH: $(echo $PATH)" >&2
                    echo "Checking /app/.venv/bin..." >&2
                    ls -la /app/.venv/bin/ | grep torch >&2 || true
                    exit 1
                  fi
                  echo "Found torchrun: ${TORCHRUN_CMD}" >&2
                  
                  # Generate unique run ID for this training session
                  {{- if .Values.elastic.etcd.runId }}
                  RUN_ID="{{ .Values.elastic.etcd.runId }}"
                  {{- else }}
                  RUN_ID="{{ include "diloco-training.jobName" . }}"
                  {{- end }}
                  echo "Using run ID: ${RUN_ID}" >&2
                  
                  echo "Launching torchrun with etcd rendezvous..." >&2
                  exec ${TORCHRUN_CMD} \
                    --nnodes={{ include "diloco-training.totalMinNodes" . }}:{{ include "diloco-training.totalMaxNodes" . }} \
                    --nproc_per_node={{ .Values.resources.gpuCount }} \
                    --max-restarts={{ .Values.elastic.maxRestarts | default 3 }} \
                    --rdzv_backend=etcd-v2 \
                    --rdzv_endpoint=${RDZV_ENDPOINT} \
                    --rdzv_id=${RUN_ID} \
                    --rdzv-conf=timeout={{ .Values.elastic.rdzvTimeout | default 600 }},protocol={{ .Values.elastic.etcd.protocol | default "http" }},prefix={{ .Values.elastic.etcd.prefix | default "/torchelastic" }} \
                    /app/diloco_training/training/start_training.py
              env:
                - name: POD_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                # HuggingFace Hub settings
                - name: HF_HUB_DOWNLOAD_TIMEOUT
                  value: "120"
                - name: HF_HUB_ETAG_TIMEOUT
                  value: "1800"
                - name: HF_HUB_DISABLE_PROGRESS_BARS
                  value: "1"
                - name: HF_HUB_VERBOSITY
                  value: "info"
              envFrom:
                - configMapRef:
                    name: {{ include "diloco-training.configmapName" . }}
              image: {{ .Values.gpu.nvidia.image }}
              imagePullPolicy: Always
              name: nvidia-worker
              resources:
                requests:
                  # PVCs can not easily be used with Volcano, so we use the storageGb value for ephemeral storage
                  ephemeral-storage: {{ .Values.resources.storageGb }}Gi
                  cpu: {{ .Values.resources.cpuCores }}
                  memory: {{ .Values.resources.memoryGb }}Gi
                  nvidia.com/gpu: {{ .Values.resources.gpuCount }}
                limits:
                  # PVCs can not easily be used with Volcano, so we use the storageGb value for ephemeral storage
                  ephemeral-storage: {{ .Values.resources.storageGb }}Gi
                  cpu: {{ .Values.resources.cpuCores }}
                  memory: {{ .Values.resources.memoryGb }}Gi
                  nvidia.com/gpu: {{ .Values.resources.gpuCount }}
              workingDir: /app
              volumeMounts:
                - mountPath: /dev/shm
                  name: dshm
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
          restartPolicy: OnFailure
{{- end }}
{{- if .Values.gpu.amd.enabled }}
    - maxRetry: 3
      minAvailable: {{ include "diloco-training.amd.minNodes" . }}
      name: amd-worker
      replicas: {{ include "diloco-training.amd.maxNodes" . }}
      template:
        metadata:
          annotations:
            scheduling.k8s.io/group-name: {{ include "diloco-training.jobName" . }}
          labels:
            job-name: {{ include "diloco-training.jobName" . }}
            workspace.exalsius.ai/database-id: {{ .Release.Name }}
        spec:
{{- if .Values.gpu.amd.runtimeClassName }}
          runtimeClassName: {{ .Values.gpu.amd.runtimeClassName }}
{{- end }}
          containers:
            - command:
                - /bin/sh
                - -c
                - |
                  set -e
                  # Ensure output is not buffered
                  export PYTHONUNBUFFERED=1
                  export TORCH_DISTRIBUTED_DEBUG=INFO
                  
                  # NCCL configuration for high-latency environments
                  export NCCL_DEBUG={{ .Values.elastic.nccl.debug | default "WARN" }}
                  export NCCL_ASYNC_ERROR_HANDLING={{ .Values.elastic.nccl.asyncErrorHandling | default 1 }}
                  export NCCL_TIMEOUT={{ .Values.elastic.nccl.timeout | default 600 }}
                  export NCCL_IB_TIMEOUT={{ .Values.elastic.nccl.ibTimeout | default 22 }}
                  export NCCL_NSOCKS_PERTHREAD={{ .Values.elastic.nccl.nsocksPerThread | default 8 }}
                  export NCCL_SOCKET_NTHREADS={{ .Values.elastic.nccl.socketNthreads | default 4 }}
                  {{- if .Values.elastic.nccl.socketIfname }}
                  export NCCL_SOCKET_IFNAME={{ .Values.elastic.nccl.socketIfname }}
                  {{- end }}
                  echo "NCCL configured for high-latency environment (DEBUG=${NCCL_DEBUG})" >&2
                  
                  echo "Starting PyTorch Elastic training..." >&2
                  echo "POD_NAME: ${POD_NAME}" >&2
                  
                  # Determine etcd rendezvous endpoint
                  # etcd provides fault-tolerant coordination for PyTorch Elastic training
                  echo "Configuring etcd rendezvous backend..." >&2
                  
                  {{- if .Values.elastic.etcd.externalEndpoint }}
                  # Using external etcd cluster
                  ETCD_ENDPOINT="{{ .Values.elastic.etcd.externalEndpoint }}"
                  echo "Using external etcd: ${ETCD_ENDPOINT}" >&2
                  {{- else if .Values.etcd.enabled }}
                  # Using embedded etcd deployed with this chart
                  ETCD_HOST="{{ include "diloco-training.etcdEndpoint" . }}"
                  ETCD_ENDPOINT="${ETCD_HOST}"
                  echo "Using embedded etcd: ${ETCD_ENDPOINT}" >&2

                  
                  # Wait for etcd to be ready
                  echo "Waiting for etcd to be ready..." >&2
                  ETCD_READY=false
                  for i in $(seq 1 30); do
                    # Try to connect to etcd health endpoint
                    if command -v curl > /dev/null 2>&1; then
                      if curl -sf --connect-timeout 2 "{{ .Values.elastic.etcd.protocol | default "http" }}://${ETCD_ENDPOINT}/health" > /dev/null 2>&1; then
                        echo "etcd is ready and healthy (via curl)" >&2
                        ETCD_READY=true
                        break
                      fi
                    elif command -v wget > /dev/null 2>&1; then
                      if wget -q --timeout=2 --tries=1 "{{ .Values.elastic.etcd.protocol | default "http" }}://${ETCD_ENDPOINT}/health" -O /dev/null 2>&1; then
                        echo "etcd is ready and healthy (via wget)" >&2
                        ETCD_READY=true
                        break
                      fi
                    elif command -v python3 > /dev/null 2>&1; then
                      if python3 -c "import urllib.request; urllib.request.urlopen('{{ .Values.elastic.etcd.protocol | default "http" }}://${ETCD_ENDPOINT}/health', timeout=2)" > /dev/null 2>&1; then
                        echo "etcd is ready and healthy (via python)" >&2
                        ETCD_READY=true
                        break
                      fi
                    fi
                    echo "Waiting for etcd to be ready... (attempt $i/30)" >&2
                    sleep 2
                  done
                  
                  if [ "$ETCD_READY" = "false" ]; then
                    echo "WARNING: Could not verify etcd availability after 30 attempts" >&2
                    echo "Continuing anyway - torchrun will handle connection retry" >&2
                  fi
                  
                  {{- else }}
                  echo "ERROR: etcd is not enabled and no external endpoint specified!" >&2
                  exit 1
                  {{- end }}
                  
                  # Use etcd endpoint (hostname:port only, path goes in rdzv-conf)
                  RDZV_ENDPOINT="${ETCD_ENDPOINT}"
                  
                  echo "etcd rendezvous endpoint: ${RDZV_ENDPOINT}" >&2
                  echo "Node rank ${NODE_RANK} ready for fault-tolerant elastic training" >&2
                  
                  # Check if torchrun exists (try multiple locations)
                  TORCHRUN_CMD=""
                  if command -v torchrun &> /dev/null; then
                    TORCHRUN_CMD="torchrun"
                  elif [ -f "/app/.venv/bin/torchrun" ]; then
                    TORCHRUN_CMD="/app/.venv/bin/torchrun"
                  elif [ -f "/usr/local/bin/torchrun" ]; then
                    TORCHRUN_CMD="/usr/local/bin/torchrun"
                  else
                    echo "ERROR: torchrun command not found!" >&2
                    echo "Available in PATH: $(echo $PATH)" >&2
                    echo "Checking /app/.venv/bin..." >&2
                    ls -la /app/.venv/bin/ | grep torch >&2 || true
                    exit 1
                  fi
                  echo "Found torchrun: ${TORCHRUN_CMD}" >&2
                  
                  # Generate unique run ID for this training session
                  {{- if .Values.elastic.etcd.runId }}
                  RUN_ID="{{ .Values.elastic.etcd.runId }}"
                  {{- else }}
                  RUN_ID="{{ include "diloco-training.jobName" . }}"
                  {{- end }}
                  echo "Using run ID: ${RUN_ID}" >&2
                  
                  echo "Launching torchrun with etcd rendezvous..." >&2
                  exec ${TORCHRUN_CMD} \
                    --nnodes={{ include "diloco-training.totalMinNodes" . }}:{{ include "diloco-training.totalMaxNodes" . }} \
                    --nproc_per_node={{ .Values.resources.gpuCount }} \
                    --max-restarts={{ .Values.elastic.maxRestarts | default 3 }} \
                    --rdzv_backend=etcd-v2 \
                    --rdzv_endpoint=${RDZV_ENDPOINT} \
                    --rdzv_id=${RUN_ID} \
                    --rdzv-conf=timeout={{ .Values.elastic.rdzvTimeout | default 600 }},protocol={{ .Values.elastic.etcd.protocol | default "http" }},prefix={{ .Values.elastic.etcd.prefix | default "/torchelastic" }} \
                    /app/diloco_training/training/start_training.py
              env:
                - name: POD_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                # HuggingFace Hub settings
                - name: HF_HUB_DOWNLOAD_TIMEOUT
                  value: "120"
                - name: HF_HUB_ETAG_TIMEOUT
                  value: "1800"
                - name: HF_HUB_DISABLE_PROGRESS_BARS
                  value: "1"
                - name: HF_HUB_VERBOSITY
                  value: "info"
              envFrom:
                - configMapRef:
                    name: {{ include "diloco-training.configmapName" . }}
              image: {{ .Values.gpu.amd.image }}
              imagePullPolicy: Always
              name: amd-worker
              resources:
                requests:
                  # PVCs can not easily be used with Volcano, so we use the storageGb value for ephemeral storage
                  ephemeral-storage: {{ .Values.resources.storageGb }}Gi
                  cpu: {{ .Values.resources.cpuCores }}
                  memory: {{ .Values.resources.memoryGb }}Gi
                  amd.com/gpu: {{ .Values.resources.gpuCount }}
                limits:
                  # PVCs can not easily be used with Volcano, so we use the storageGb value for ephemeral storage
                  ephemeral-storage: {{ .Values.resources.storageGb }}Gi
                  cpu: {{ .Values.resources.cpuCores }}
                  memory: {{ .Values.resources.memoryGb }}Gi
                  amd.com/gpu: {{ .Values.resources.gpuCount }}
              workingDir: /app
              volumeMounts:
                - mountPath: /dev/shm
                  name: dshm
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
          restartPolicy: OnFailure
{{- end }}

