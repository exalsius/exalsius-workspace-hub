apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: {{ include "diloco-training.jobName" . }}
  namespace: {{ .Values.global.deploymentNamespace }}
  labels:
    job-name: {{ include "diloco-training.jobName" . }}
spec:
  maxRetry: 3
  minAvailable: {{ include "diloco-training.totalMinNodes" . }}
  plugins:
    svc: []
  queue: default
  schedulerName: volcano
  tasks:
{{- if .Values.gpu.nvidia.enabled }}
    - maxRetry: 3
      minAvailable: {{ include "diloco-training.nvidia.minNodes" . }}
      name: nvidia-worker
      replicas: {{ include "diloco-training.nvidia.maxNodes" . }}
      template:
        metadata:
          annotations:
            scheduling.k8s.io/group-name: {{ include "diloco-training.jobName" . }}
          labels:
            job-name: {{ include "diloco-training.jobName" . }}
            workspace.exalsius.ai/database-id: {{ .Release.Name }}
        spec:
          runtimeClassName: {{ .Values.gpu.nvidia.runtimeClassName }}
          containers:
            - command:
                - /bin/sh
                - -c
                - |
                  set -e
                  # Ensure output is not buffered
                  export PYTHONUNBUFFERED=1
                  
                  # NCCL configuration for high-latency environments
                  export NCCL_DEBUG={{ .Values.elastic.nccl.debug | default "WARN" }}
                  export NCCL_ASYNC_ERROR_HANDLING={{ .Values.elastic.nccl.asyncErrorHandling | default 1 }}
                  export NCCL_TIMEOUT={{ .Values.elastic.nccl.timeout | default 600 }}
                  export NCCL_IB_TIMEOUT={{ .Values.elastic.nccl.ibTimeout | default 22 }}
                  export NCCL_NSOCKS_PERTHREAD={{ .Values.elastic.nccl.nsocksPerThread | default 8 }}
                  export NCCL_SOCKET_NTHREADS={{ .Values.elastic.nccl.socketNthreads | default 4 }}
                  {{- if .Values.elastic.nccl.socketIfname }}
                  export NCCL_SOCKET_IFNAME={{ .Values.elastic.nccl.socketIfname }}
                  {{- end }}
                  echo "NCCL configured for high-latency environment (DEBUG=${NCCL_DEBUG})" >&2
                  
                  echo "Starting PyTorch Elastic training..." >&2
                  echo "POD_NAME: ${POD_NAME}" >&2
                  
                  # Extract node rank from pod name (format: {job-name}-worker-{index})
                  # Volcano creates pods with names like: {job-name}-worker-0, {job-name}-worker-1, etc.
                  NODE_RANK=$(echo ${POD_NAME} | grep -oE '[0-9]+$' || echo "0")
                  echo "Extracted NODE_RANK: ${NODE_RANK}" >&2
                  
                  # Determine etcd rendezvous endpoint
                  # etcd provides fault-tolerant coordination for PyTorch Elastic training
                  echo "Configuring etcd rendezvous backend..." >&2
                  
                  {{- if .Values.elastic.etcd.externalEndpoint }}
                  # Using external etcd cluster
                  ETCD_ENDPOINT="{{ .Values.elastic.etcd.externalEndpoint }}"
                  echo "Using external etcd: ${ETCD_ENDPOINT}" >&2
                  {{- else if .Values.etcd.enabled }}
                  # Using embedded etcd deployed with this chart
                  ETCD_HOST="{{ include "diloco-training.etcdEndpoint" . }}"
                  ETCD_ENDPOINT="${ETCD_HOST}"
                  echo "Using embedded etcd: ${ETCD_ENDPOINT}" >&2


                  #  ucc stuff
                  #export TORCH_CUDA_ARCH_LIST="Turing"
                  #export USE_MPI=1
                  #export BUILD_TEST=0
                  #export BUILD_TEST_LIBTORCH=0
                  export USE_UCC=ON
                  export USE_SYSTEM_UCC=ON
                  export USE_DISTRIBUTED=ON
                  export USE_C10D_UCC=ON
                  #export USE_C10D_MPI=ON
                  #export TORCH_BLAS_PREFER_CUBLASLT=0
                  export USE_ROCM=OFF
                  export USE_CUDA=ON
                  export USE_MKLDNN=OFF

                  
                  # Wait for etcd to be ready
                  echo "Waiting for etcd to be ready..." >&2
                  for i in $(seq 1 30); do
                    if command -v getent > /dev/null 2>&1; then
                      if getent hosts {{ include "diloco-training.etcdEndpoint" . }} > /dev/null 2>&1; then
                        echo "etcd service resolved successfully" >&2
                        break
                      fi
                    fi
                    echo "Waiting for etcd DNS... (attempt $i/30)" >&2
                    sleep 2
                  done
                  {{- else }}
                  echo "ERROR: etcd is not enabled and no external endpoint specified!" >&2
                  exit 1
                  {{- end }}
                  
                  # Use etcd endpoint (hostname:port only, path goes in rdzv-conf)
                  RDZV_ENDPOINT="${ETCD_ENDPOINT}"
                  
                  echo "etcd rendezvous endpoint: ${RDZV_ENDPOINT}" >&2
                  echo "Node rank ${NODE_RANK} ready for fault-tolerant elastic training" >&2
                  
                  # Check if torchrun exists (try multiple locations)
                  TORCHRUN_CMD=""
                  if command -v torchrun &> /dev/null; then
                    TORCHRUN_CMD="torchrun"
                  elif [ -f "/app/.venv/bin/torchrun" ]; then
                    TORCHRUN_CMD="/app/.venv/bin/torchrun"
                  elif [ -f "/usr/local/bin/torchrun" ]; then
                    TORCHRUN_CMD="/usr/local/bin/torchrun"
                  else
                    echo "ERROR: torchrun command not found!" >&2
                    echo "Available in PATH: $(echo $PATH)" >&2
                    echo "Checking /app/.venv/bin..." >&2
                    ls -la /app/.venv/bin/ | grep torch >&2 || true
                    exit 1
                  fi
                  echo "Found torchrun: ${TORCHRUN_CMD}" >&2
                  
                  echo "Launching torchrun with etcd rendezvous..." >&2
                  exec ${TORCHRUN_CMD} \
                    --nnodes={{ include "diloco-training.totalMinNodes" . }}:{{ include "diloco-training.totalMaxNodes" . }} \
                    --nproc_per_node={{ .Values.gpuCountPerNode }} \
                    --max-restarts={{ .Values.elastic.maxRestarts | default 3 }} \
                    --node_rank=${NODE_RANK} \
                    --rdzv_backend=etcd \
                    --rdzv_endpoint=${RDZV_ENDPOINT} \
                    --rdzv-conf=timeout={{ .Values.elastic.rdzvTimeout | default 600 }},protocol={{ .Values.elastic.etcd.protocol | default "http" }},prefix={{ .Values.elastic.etcd.prefix | default "/torchelastic" }}/{{ include "diloco-training.jobName" . }} \
                    /app/diloco_training/training/start_training.py
              env:
                - name: POD_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
              envFrom:
                - configMapRef:
                    name: {{ include "diloco-training.configmapName" . }}
              image: {{ .Values.gpu.nvidia.image }}
              imagePullPolicy: Always
              name: nvidia-worker
              resources:
                requests:
                  cpu: {{ .Values.cpuCores }}
                  memory: {{ .Values.memoryGb }}Gi
                  nvidia.com/gpu: {{ .Values.gpuCountPerNode }}
                  ephemeral-storage: {{ .Values.ephemeralStorageGb }}Gi
                limits:
                  cpu: {{ .Values.cpuCores }}
                  memory: {{ .Values.memoryGb }}Gi
                  nvidia.com/gpu: {{ .Values.gpuCountPerNode }}
                  ephemeral-storage: {{ .Values.ephemeralStorageGb }}Gi
              workingDir: /app
          restartPolicy: OnFailure
{{- end }}
{{- if .Values.gpu.amd.enabled }}
    - maxRetry: 3
      minAvailable: {{ include "diloco-training.amd.minNodes" . }}
      name: amd-worker
      replicas: {{ include "diloco-training.amd.maxNodes" . }}
      template:
        metadata:
          annotations:
            scheduling.k8s.io/group-name: {{ include "diloco-training.jobName" . }}
          labels:
            job-name: {{ include "diloco-training.jobName" . }}
            workspace.exalsius.ai/database-id: {{ .Release.Name }}
        spec:
{{- if .Values.gpu.amd.runtimeClassName }}
          runtimeClassName: {{ .Values.gpu.amd.runtimeClassName }}
{{- end }}
          containers:
            - command:
                - /bin/sh
                - -c
                - |
                  set -e
                  # Ensure output is not buffered
                  export PYTHONUNBUFFERED=1
                  
                  # NCCL configuration for high-latency environments
                  export NCCL_DEBUG={{ .Values.elastic.nccl.debug | default "WARN" }}
                  export NCCL_ASYNC_ERROR_HANDLING={{ .Values.elastic.nccl.asyncErrorHandling | default 1 }}
                  export NCCL_TIMEOUT={{ .Values.elastic.nccl.timeout | default 600 }}
                  export NCCL_IB_TIMEOUT={{ .Values.elastic.nccl.ibTimeout | default 22 }}
                  export NCCL_NSOCKS_PERTHREAD={{ .Values.elastic.nccl.nsocksPerThread | default 8 }}
                  export NCCL_SOCKET_NTHREADS={{ .Values.elastic.nccl.socketNthreads | default 4 }}
                  {{- if .Values.elastic.nccl.socketIfname }}
                  export NCCL_SOCKET_IFNAME={{ .Values.elastic.nccl.socketIfname }}
                  {{- end }}
                  echo "NCCL configured for high-latency environment (DEBUG=${NCCL_DEBUG})" >&2
                  
                  echo "Starting PyTorch Elastic training..." >&2
                  echo "POD_NAME: ${POD_NAME}" >&2
                  
                  # Extract node rank from pod name (format: {job-name}-worker-{index})
                  # Volcano creates pods with names like: {job-name}-worker-0, {job-name}-worker-1, etc.
                  NODE_RANK=$(echo ${POD_NAME} | grep -oE '[0-9]+$' || echo "0")
                  echo "Extracted NODE_RANK: ${NODE_RANK}" >&2
                  
                  # Determine etcd rendezvous endpoint
                  # etcd provides fault-tolerant coordination for PyTorch Elastic training
                  echo "Configuring etcd rendezvous backend..." >&2
                  
                  {{- if .Values.elastic.etcd.externalEndpoint }}
                  # Using external etcd cluster
                  ETCD_ENDPOINT="{{ .Values.elastic.etcd.externalEndpoint }}"
                  echo "Using external etcd: ${ETCD_ENDPOINT}" >&2
                  {{- else if .Values.etcd.enabled }}
                  # Using embedded etcd deployed with this chart
                  ETCD_HOST="{{ include "diloco-training.etcdEndpoint" . }}"
                  ETCD_ENDPOINT="${ETCD_HOST}"
                  echo "Using embedded etcd: ${ETCD_ENDPOINT}" >&2

                  #  ucc stuff
                  #export TORCH_CUDA_ARCH_LIST="Turing"
                  #export USE_MPI=1
                  #export BUILD_TEST=0
                  #export BUILD_TEST_LIBTORCH=0
                  export USE_UCC=ON
                  export USE_SYSTEM_UCC=ON
                  export USE_DISTRIBUTED=ON
                  export USE_C10D_UCC=ON
                  #export USE_C10D_MPI=ON
                  #export TORCH_BLAS_PREFER_CUBLASLT=0
                  export USE_ROCM=ON
                  export USE_CUDA=OFF
                  export USE_MKLDNN=OFF
                  
                  # Wait for etcd to be ready
                  echo "Waiting for etcd to be ready..." >&2
                  for i in $(seq 1 30); do
                    if command -v getent > /dev/null 2>&1; then
                      if getent hosts {{ include "diloco-training.etcdEndpoint" . }} > /dev/null 2>&1; then
                        echo "etcd service resolved successfully" >&2
                        break
                      fi
                    fi
                    echo "Waiting for etcd DNS... (attempt $i/30)" >&2
                    sleep 2
                  done
                  {{- else }}
                  echo "ERROR: etcd is not enabled and no external endpoint specified!" >&2
                  exit 1
                  {{- end }}
                  
                  # Use etcd endpoint (hostname:port only, path goes in rdzv-conf)
                  RDZV_ENDPOINT="${ETCD_ENDPOINT}"
                  
                  echo "etcd rendezvous endpoint: ${RDZV_ENDPOINT}" >&2
                  echo "Node rank ${NODE_RANK} ready for fault-tolerant elastic training" >&2
                  
                  # Check if torchrun exists (try multiple locations)
                  TORCHRUN_CMD=""
                  if command -v torchrun &> /dev/null; then
                    TORCHRUN_CMD="torchrun"
                  elif [ -f "/app/.venv/bin/torchrun" ]; then
                    TORCHRUN_CMD="/app/.venv/bin/torchrun"
                  elif [ -f "/usr/local/bin/torchrun" ]; then
                    TORCHRUN_CMD="/usr/local/bin/torchrun"
                  else
                    echo "ERROR: torchrun command not found!" >&2
                    echo "Available in PATH: $(echo $PATH)" >&2
                    echo "Checking /app/.venv/bin..." >&2
                    ls -la /app/.venv/bin/ | grep torch >&2 || true
                    exit 1
                  fi
                  echo "Found torchrun: ${TORCHRUN_CMD}" >&2
                  
                  echo "Launching torchrun with etcd rendezvous..." >&2
                  exec ${TORCHRUN_CMD} \
                    --nnodes={{ include "diloco-training.totalMinNodes" . }}:{{ include "diloco-training.totalMaxNodes" . }} \
                    --nproc_per_node={{ .Values.gpuCountPerNode }} \
                    --max-restarts={{ .Values.elastic.maxRestarts | default 3 }} \
                    --node_rank=${NODE_RANK} \
                    --rdzv_backend=etcd \
                    --rdzv_endpoint=${RDZV_ENDPOINT} \
                    --rdzv-conf=timeout={{ .Values.elastic.rdzvTimeout | default 600 }},protocol={{ .Values.elastic.etcd.protocol | default "http" }},prefix={{ .Values.elastic.etcd.prefix | default "/torchelastic" }}/{{ include "diloco-training.jobName" . }} \
                    /app/diloco_training/training/start_training.py
              env:
                - name: POD_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
              envFrom:
                - configMapRef:
                    name: {{ include "diloco-training.configmapName" . }}
              image: {{ .Values.gpu.amd.image }}
              imagePullPolicy: Always
              name: amd-worker
              resources:
                requests:
                  cpu: {{ .Values.cpuCores }}
                  memory: {{ .Values.memoryGb }}Gi
                  amd.com/gpu: {{ .Values.gpuCountPerNode }}
                  ephemeral-storage: {{ .Values.ephemeralStorageGb }}Gi
                limits:
                  cpu: {{ .Values.cpuCores }}
                  memory: {{ .Values.memoryGb }}Gi
                  amd.com/gpu: {{ .Values.gpuCountPerNode }}
                  ephemeral-storage: {{ .Values.ephemeralStorageGb }}Gi
              workingDir: /app
          restartPolicy: OnFailure
{{- end }}

