# Test Workspace Helm Chart Values
global:
  deploymentName: "diloco-training-job"
  deploymentNamespace: "default"

# DO NOT CHANGE THIS PARAMETER
deploymentNumReplicas: 1

# PyTorch Elastic training configuration with etcd rendezvous
elastic:
  minNodes: 2  # Minimum number of nodes for elastic training
  maxNodes: 3  # Maximum number of nodes for elastic training
  maxRestarts: 5  # Maximum restarts for elastic training
  
  # GPU distribution policy (used when gpu.*.minNodes/maxNodes are null)
  # Options: "auto" (split evenly), "prefer-nvidia" (NVIDIA first, AMD fallback), "prefer-amd" (AMD first, NVIDIA fallback)
  gpuDistribution: "auto"
  
  # etcd rendezvous backend configuration
  etcd:
    # External etcd configuration (if using external etcd instead of the embedded subchart)
    externalEndpoint: ""  # e.g., "etcd.infrastructure.svc.cluster.local:2379"
    prefix: "/torchelastic"  # etcd key prefix for job isolation
    protocol: "http"         # "http" or "https"

# GPU-specific configurations
# If explicitly set, these override the elastic.minNodes/maxNodes/gpuDistribution
# If not set (or values are null), they will be calculated from elastic config
gpu:
  nvidia:
    enabled: true
    image: "ghcr.io/exalsius/diloco-training:cuda-dev"
    minNodes: null  # null means: calculate from elastic.minNodes + gpuDistribution
    maxNodes: null  # null means: calculate from elastic.maxNodes + gpuDistribution
    runtimeClassName: "nvidia"
  amd:
    enabled: true
    image: "ghcr.io/exalsius/diloco-training:rocm-dev"
    minNodes: null  # null means: calculate from elastic.minNodes + gpuDistribution
    maxNodes: null  # null means: calculate from elastic.maxNodes + gpuDistribution
    runtimeClassName: ""  # Empty for default runtime

# Ephemeral storage configuration
ephemeralStorageGb: 50

# Resource configuration for a single worker pod
cpuCores: 6
memoryGb: 12
gpuCountPerNode: 1  # Number of GPUs per node (also determines PyTorch processes per node)

# DiLoCo Training Environment Variables
diloco:
  model: "gpt-neo-x"
  dataset: "c4"
  localSteps: "50"
  lr: "4e-4"
  outerLr: "0.7"
  warmupSteps: "500"
  totalSteps: "1000"
  perDeviceTrainBatchSize: "64"
  batchSize: "512"
  optimMethod: "demo"
  quantization: "false"
  asyncCommunication: "false"
  checkpointPath: "checkpoint.pth"
  checkpointInterval: "512"
  device: "cuda"
  wandbProjectName: "diloco-training"
  wandbGroup: "diloco-gptneo-c4-elastic"
  wandbRunId: "gpt-4e-4"
  heterogeneous: "true"
  minBatchSize: "32"
  maxBatchSize: "512"
  groupPercVariance: "0.15"
  compressionDecay: "0.95"
  compressionTopk: "32"
  experimentDescription: "GPT training on c4 using DiLoCo distributed training"
  experimentTags: "[\"diloco\", \"gpt-neo-x\", \"c4\"]"
  seed: "42"
  wandbLogging: "true"
  wandbUserKey: "XXXXXXXXXXXXXXXXXXXXXXXXXXX"
  huggingfaceToken: "XXXXXXXXXXXXXXXXXXXXXXXX"
  compileModel: "false"
  compileBackend: "inductor"
  compileMode: "default"
  hfUpload: "false"
  trainedModelHfName: ""

# etcd subchart configuration (bitnami/etcd)
# Set enabled: false to use an external etcd cluster (configure elastic.etcd.externalEndpoint)
etcd:
  enabled: true
  replicaCount: 1
  image:
    registry: docker.io
    repository: bitnamilegacy/etcd
  auth:
    rbac:
      create: false
      rootPassword: ""
    client:
      secureTransport: false
      enableAuthentication: false
  livenessProbe:
    enabled: false  # Disabled to prevent restart loops
  readinessProbe:
    enabled: false  # Disabled to prevent restart loops
  extraEnvVars:
    - name: ETCD_ENABLE_V2
      value: "true"