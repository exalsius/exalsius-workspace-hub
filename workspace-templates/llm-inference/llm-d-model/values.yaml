ip:
  inferenceExtension:
    replicas: 1
    flags:
      kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
    image:
      name: llm-d-inference-scheduler
      hub: ghcr.io/llm-d
      tag: v0.5.0
      pullPolicy: Always
    extProcPort: 9002
    pluginsConfigFile: "default-plugins.yaml"
    monitoring:
      interval: "10s"
      prometheus:
        enabled: true
        auth:
          enabled: false
  provider:
    name: istio
  inferencePool:
    targetPorts:
      - number: 8000
    modelServerType: vllm
    modelServers:
      ### CHANGES REQUIRED ###
      matchLabels:
        llm-d.ai/model: Qwen3-1.7B
      ### CHANGES REQUIRED ###

### CHANGES REQUIRED ###
huggingfaceToken: "my-hugging-face-token"
### CHANGES REQUIRED ###

# The chart automatically calculates accelerator resources (e.g., nvidia.com/gpu) based on parallelism settings.
# However, you can override this by explicitly setting resources in the container spec.
# 1) If you explicitly set resources.limits.<accelerator> in the container spec, that value is used.
# 2) Otherwise, the value is auto-calculated from parallelism.tensor * parallelism.dataLocal.
# --> If you use this strategy, vLLM parallelism arguments like --tensor-parallel-size and --data-parallel-size-local and the corresponding value will be added automatically to the first container.
ms:
  multinode: false
  modelArtifacts:
    size: 20Gi
    authSecretName: "llm-d-hf-token"
    ### CHANGES REQUIRED ###
    uri: "hf://Qwen/Qwen3-1.7B"
    name: "Qwen/Qwen3-1.7B"
    labels:
      llm-d.ai/model: Qwen3-1.7B
    ### CHANGES REQUIRED ###
  routing:
    proxy:
      enabled: false
      targetPort: 8000
  accelerator:
    type: nvidia
  decode:
    create: true
    replicas: 1
    # VLLM parallelism configuration
    parallelism:
      tensor: 1
      data: 1
      dataLocal: 1
      workers: 1
    monitoring:
      podmonitor:
        enabled: true
        portName: "vllm"
        path: "/metrics"
        interval: "30s"
    containers:
    - name: "vllm"
      image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
      modelCommand: vllmServe
      args:
        - "--disable-uvicorn-access-log"
        - "--gpu-memory-utilization=0.95"
      env:
        - name: VLLM_USE_FLASHINFER_SAMPLER
          value: "0"
      ports:
        - containerPort: 8000
          name: vllm
          protocol: TCP
      mountModelVolume: true
      volumeMounts:
        - name: metrics-volume
          mountPath: /.config
        - name: shm
          mountPath: /dev/shm
        - name: torch-compile-cache
          mountPath: /.cache
      startupProbe:
        httpGet:
          path: /v1/models
          port: vllm
        initialDelaySeconds: 15
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 60
      livenessProbe:
        httpGet:
          path: /health
          port: vllm
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      readinessProbe:
        httpGet:
          path: /v1/models
          port: vllm
        periodSeconds: 5
        timeoutSeconds: 2
        failureThreshold: 3
    volumes:
    - name: metrics-volume
      emptyDir: {}
    - name: torch-compile-cache
      emptyDir: {}
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 20Gi
  # Disabled here, but mostly same configuration options as for decode component
  prefill:
    create: false