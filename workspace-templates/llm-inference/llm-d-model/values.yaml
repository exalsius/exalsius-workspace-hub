ip:
  inferenceExtension:
    replicas: 1
    flags:
      kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
    image:
      name: llm-d-inference-scheduler
      hub: ghcr.io/llm-d
      tag: v0.5.0
      pullPolicy: Always
    extProcPort: 9002
    pluginsConfigFile: "default-plugins.yaml"
    monitoring:
      interval: "10s"
      prometheus:
        enabled: true
        auth:
          enabled: false
  provider:
    name: istio
  inferencePool:
    targetPorts:
      - number: 8000
    modelServerType: vllm
    modelServers:
      ### CHANGES REQUIRED ###
      matchLabels:
        llm-d.ai/model: Qwen3-1.7B
      ### CHANGES REQUIRED ###
        llm-d.ai/inferenceServing: "true"

### CHANGES REQUIRED ###
huggingfaceToken: "my-hugging-face-token"
### CHANGES REQUIRED ###

ms:
  multinode: false
  modelArtifacts:
    size: 20Gi
    authSecretName: "llm-d-hf-token"
    ### CHANGES REQUIRED ###
    uri: "hf://Qwen/Qwen3-1.7B"
    name: "Qwen/Qwen3-1.7B"
    labels:
      llm-d.ai/model: Qwen3-1.7B
    ### CHANGES REQUIRED ###
      llm-d.ai/inferenceServing: "true"
  routing:
    proxy:
      enabled: false
      targetPort: 8000
  accelerator:
    type: nvidia
  decode:
    create: true
    replicas: 1
    monitoring:
      podmonitor:
        enabled: true
        portName: "vllm"
        path: "/metrics"
        interval: "30s"
    containers:
    - name: "vllm"
      image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
      modelCommand: vllmServe
      args:
        - "--disable-uvicorn-access-log"
        - "--gpu-memory-utilization=0.95"
      env:
        - name: VLLM_USE_FLASHINFER_SAMPLER
          value: "0"
      ports:
        - containerPort: 8000
          name: vllm
          protocol: TCP
      mountModelVolume: true
      volumeMounts:
        - name: metrics-volume
          mountPath: /.config
        - name: shm
          mountPath: /dev/shm
        - name: torch-compile-cache
          mountPath: /.cache
      startupProbe:
        httpGet:
          path: /v1/models
          port: vllm
        initialDelaySeconds: 15
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 60
      livenessProbe:
        httpGet:
          path: /health
          port: vllm
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      readinessProbe:
        httpGet:
          path: /v1/models
          port: vllm
        periodSeconds: 5
        timeoutSeconds: 2
        failureThreshold: 3
    volumes:
    - name: metrics-volume
      emptyDir: {}
    - name: torch-compile-cache
      emptyDir: {}
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 20Gi
  prefill:
    create: false