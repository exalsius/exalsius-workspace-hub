ip:
  inferenceExtension:
    replicas: 1
    flags:
      kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
    image:
      name: llm-d-inference-scheduler
      hub: ghcr.io/llm-d
      tag: v0.4.0
      pullPolicy: Always
    extProcPort: 9002
    pluginsConfigFile: "default-plugins.yaml"
    monitoring:
      interval: "10s"
      prometheus:
        enabled: true
        auth:
          enabled: false
  provider:
    name: istio
  inferencePool:
    targetPorts:
      - number: 8000
    modelServerType: vllm
    modelServers:
      ### CHANGES REQUIRED ###
      matchLabels:
        llm-d.ai/model: Qwen3-1.7B
      ### CHANGES REQUIRED ###
        llm-d.ai/inferenceServing: "true"

### CHANGES REQUIRED ###
huggingfaceToken: "my-hugging-face-token"
### CHANGES REQUIRED ###

ms:
  multinode: false
  modelArtifacts:
    size: 20Gi
    authSecretName: "llm-d-hf-token"
    ### CHANGES REQUIRED ###
    uri: "hf://Qwen/Qwen3-1.7B"
    name: "Qwen/Qwen3-1.7B"
    labels:
      llm-d.ai/model: Qwen3-1.7B
    ### CHANGES REQUIRED ###
      llm-d.ai/inferenceServing: "true"
  routing:
    servicePort: 8000
    proxy:
      image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.4.0-rc.1
      connector: nixlv2
      secure: false
      zapLogLevel: debug
  decode:
    create: true
    replicas: 1
    monitoring:
      podmonitor:
        enabled: true
        portName: "metrics"
        path: "/metrics"
        interval: "30s"
    containers:
    - name: "vllm"
      image: ghcr.io/llm-d/llm-d-cuda:v0.3.1
      modelCommand: vllmServe
      args:
        - "--kv-transfer-config"
        - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
        - "--disable-uvicorn-access-log"
      env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: UCX_TLS
          value: "cuda_ipc,cuda_copy,tcp"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "5557"
        - name: VLLM_LOGGING_LEVEL
          value: INFO
        - name: VLLM_USE_FLASHINFER_SAMPLER
          value: "0"
      ports:
        - containerPort: 5557
          protocol: TCP
        - containerPort: 8200
          name: metrics
          protocol: TCP
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"
      mountModelVolume: true
      volumeMounts:
      - name: metrics-volume
        mountPath: /.config
      - name: torch-compile-cache
        mountPath: /.cache
      startupProbe:
        httpGet:
          path: /v1/models
          port: 8200
        initialDelaySeconds: 15
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 60
      livenessProbe:
        httpGet:
          path: /health
          port: 8200
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      readinessProbe:
        httpGet:
          path: /v1/models
          port: 8200
        periodSeconds: 5
        timeoutSeconds: 2
        failureThreshold: 3
    volumes:
    - name: metrics-volume
      emptyDir: {}
    - name: torch-compile-cache
      emptyDir: {}
  prefill:
    create: false